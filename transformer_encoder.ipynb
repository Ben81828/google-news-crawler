{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "   BertTokenizerFast,\n",
    "   AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "# masked language model (ALBERT, BERT)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "pre_train_model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese') # or other models above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding空間維度\n",
    "d_model = pre_train_model.config.hidden_size\n",
    "# 字典長度\n",
    "n_token = tokenizer.vocab_size\n",
    "# 類別數\n",
    "catogories = 2\n",
    "# 一句話進model的token數\n",
    "seq_len = 20\n",
    "# batch size\n",
    "batch_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def get_pretrained_embed_matrix(input_txts:List[str], seq_len:int):\n",
    "\n",
    "    # 取得input_ids\n",
    "    input_ids = tokenizer(input_txts ,return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "\n",
    "\n",
    "    if input_ids.size()[1] < seq_len:\n",
    "        # 補到長度為20\n",
    "        input_ids = torch.cat((input_ids, torch.zeros((input_ids.size()[0], seq_len - input_ids.size()[1]), dtype=torch.long)), dim=1)\n",
    "        \n",
    "    elif input_ids.size()[1] > seq_len:\n",
    "        # 截斷到長度為20\n",
    "        input_ids = input_ids[:, :seq_len]\n",
    "\n",
    "    # 查看model.transformer有哪些屬性\n",
    "    embedding_matrix = pre_train_model.get_input_embeddings().weight.data[input_ids].squeeze() \n",
    "\n",
    "    # shape: [batch_size, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "    embedding_matrix = torch.transpose(embedding_matrix, 0, 1)\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor, softmax, argmax\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, catogories: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) \n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = get_pretrained_embed_matrix\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, catogories)\n",
    "        # self.linear_test = nn.Linear(d_model, catogories)\n",
    "\n",
    "\n",
    "    def forward(self, src_texts: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, catogories]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(input_txts=src_texts, seq_len=seq_len)\n",
    "\n",
    "        src = src* math.sqrt(self.d_model) \n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            # src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src))\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        \n",
    "        output = torch.mean(output, dim=0)\n",
    "        output = self.linear(output)\n",
    "        # 過個softmax\n",
    "        output = softmax(output, dim=1)\n",
    "        # 取機率較大的那個類別\n",
    "        # output = argmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 自定义数据集类(for dataloader用)\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.outputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "# 讀資料\n",
    "data_csv = pd.read_csv(\"content_df.csv\").dropna().sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "y = data_csv[\"Click\"]\n",
    "X = data_csv[\"title\"]\n",
    "# X = data_csv['Click'].apply(lambda label: \"妳好嗎\" if label == 1 else \"天氣不錯\") # 測試\n",
    "cutpoint = len(y)//10*8 # 切8成當訓練資料\n",
    "y_train, y_test = y[:cutpoint], y[cutpoint:]\n",
    "X_train, X_test = X[:cutpoint], X[cutpoint:]\n",
    "\n",
    "# weight\n",
    "true_weight = len(y_train)/(y_train==1).sum()\n",
    "false_weight = len(y_train)/(y_train==0).sum()\n",
    "train_weights = y_train.apply(lambda label: true_weight if label == 1 else false_weight).to_list()\n",
    "\n",
    "# # 定義訓練時的sampler\n",
    "weights = [len(y_train)/(y_train==0).sum(), len(y_train)/(y_train==1).sum()]\n",
    "sample_set_nums = ((y_train==1).sum())*1 if ((y_train==1).sum())*1 < len(y_train) else len(y_train)\n",
    "# sample_set_nums = X_train.shape[0]\n",
    "train_sampler = WeightedRandomSampler(weights=train_weights, num_samples=int(sample_set_nums), replacement=False)\n",
    "\n",
    "# # 创建数据集和数据加载器\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "\n",
    "# # 定義訓練時的dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=len(y_test), shuffle=False) #驗證時不用隨機丟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(catogories=catogories, d_model=d_model, nhead=1, d_hid=128, nlayers=1, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# lr = 5.0  # learning rate\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | batch     5 | lr 0.00 | ms/batch 1046.00 | loss  0.72 | ppl     2.06\n",
      "test f1:  0.18518518518518517\n",
      "| epoch   2 | batch     5 | lr 0.00 | ms/batch 1026.04 | loss  0.67 | ppl     1.96\n",
      "test f1:  0.4011857707509881\n",
      "| epoch   3 | batch     5 | lr 0.00 | ms/batch 1079.42 | loss  0.63 | ppl     1.88\n",
      "test f1:  0.5379609544468547\n",
      "| epoch   4 | batch     5 | lr 0.00 | ms/batch 999.78 | loss  0.61 | ppl     1.83\n",
      "test f1:  0.5536159600997507\n",
      "| epoch   5 | batch     5 | lr 0.00 | ms/batch 1044.38 | loss  0.59 | ppl     1.80\n",
      "test f1:  0.5381460213289582\n",
      "| epoch   6 | batch     5 | lr 0.00 | ms/batch 1054.55 | loss  0.59 | ppl     1.80\n",
      "test f1:  0.5594149908592322\n",
      "| epoch   7 | batch     5 | lr 0.00 | ms/batch 1024.93 | loss  0.58 | ppl     1.78\n",
      "test f1:  0.5742424242424241\n",
      "| epoch   8 | batch     5 | lr 0.00 | ms/batch 1052.21 | loss  0.58 | ppl     1.79\n",
      "test f1:  0.5815602836879432\n",
      "| epoch   9 | batch     5 | lr 0.00 | ms/batch 1105.34 | loss  0.57 | ppl     1.77\n",
      "test f1:  0.5712260807937632\n",
      "| epoch  10 | batch     5 | lr 0.00 | ms/batch 1088.84 | loss  0.56 | ppl     1.76\n",
      "test f1:  0.591044776119403\n",
      "| epoch  11 | batch     5 | lr 0.00 | ms/batch 1032.50 | loss  0.56 | ppl     1.75\n",
      "test f1:  0.5798192771084338\n",
      "| epoch  12 | batch     5 | lr 0.00 | ms/batch 1030.73 | loss  0.56 | ppl     1.76\n",
      "test f1:  0.5993690851735015\n",
      "| epoch  13 | batch     5 | lr 0.00 | ms/batch 1016.55 | loss  0.56 | ppl     1.74\n",
      "test f1:  0.5883959044368601\n",
      "| epoch  14 | batch     5 | lr 0.00 | ms/batch 1022.09 | loss  0.56 | ppl     1.75\n",
      "test f1:  0.6031537450722734\n",
      "| epoch  15 | batch     5 | lr 0.00 | ms/batch 1016.90 | loss  0.55 | ppl     1.73\n",
      "test f1:  0.6060606060606061\n",
      "| epoch  16 | batch     5 | lr 0.00 | ms/batch 1060.57 | loss  0.54 | ppl     1.72\n",
      "test f1:  0.60498687664042\n",
      "| epoch  17 | batch     5 | lr 0.00 | ms/batch 1073.76 | loss  0.55 | ppl     1.73\n",
      "test f1:  0.608187134502924\n",
      "| epoch  18 | batch     5 | lr 0.00 | ms/batch 1023.88 | loss  0.55 | ppl     1.74\n",
      "test f1:  0.6008119079837618\n",
      "| epoch  19 | batch     5 | lr 0.00 | ms/batch 1019.53 | loss  0.55 | ppl     1.73\n",
      "test f1:  0.608469055374593\n",
      "| epoch  20 | batch     5 | lr 0.00 | ms/batch 1007.60 | loss  0.54 | ppl     1.71\n",
      "test f1:  0.6124590163934426\n",
      "| epoch  21 | batch     5 | lr 0.00 | ms/batch 1042.61 | loss  0.54 | ppl     1.71\n",
      "test f1:  0.610738255033557\n",
      "| epoch  22 | batch     5 | lr 0.00 | ms/batch 1049.92 | loss  0.55 | ppl     1.72\n",
      "test f1:  0.6137499999999999\n",
      "| epoch  23 | batch     5 | lr 0.00 | ms/batch 1002.50 | loss  0.53 | ppl     1.70\n",
      "test f1:  0.6228419654714474\n",
      "| epoch  24 | batch     5 | lr 0.00 | ms/batch 1095.86 | loss  0.54 | ppl     1.72\n",
      "test f1:  0.6256914566687153\n",
      "| epoch  25 | batch     5 | lr 0.00 | ms/batch 1017.35 | loss  0.53 | ppl     1.70\n",
      "test f1:  0.6129692832764505\n",
      "| epoch  26 | batch     5 | lr 0.00 | ms/batch 1051.78 | loss  0.54 | ppl     1.71\n",
      "test f1:  0.6298543689320389\n",
      "| epoch  27 | batch     5 | lr 0.00 | ms/batch 1059.67 | loss  0.52 | ppl     1.69\n",
      "test f1:  0.6145764937623112\n",
      "| epoch  28 | batch     5 | lr 0.00 | ms/batch 1064.45 | loss  0.52 | ppl     1.68\n",
      "test f1:  0.6210937500000001\n",
      "| epoch  29 | batch     5 | lr 0.00 | ms/batch 1061.95 | loss  0.52 | ppl     1.68\n",
      "test f1:  0.6278026905829597\n",
      "| epoch  30 | batch     5 | lr 0.00 | ms/batch 1117.01 | loss  0.52 | ppl     1.67\n",
      "test f1:  0.6368785399622404\n",
      "| epoch  31 | batch     5 | lr 0.00 | ms/batch 1071.12 | loss  0.52 | ppl     1.68\n",
      "test f1:  0.6356300434512725\n",
      "| epoch  32 | batch     5 | lr 0.00 | ms/batch 1015.95 | loss  0.51 | ppl     1.67\n",
      "test f1:  0.6305353602115004\n",
      "| epoch  33 | batch     5 | lr 0.00 | ms/batch 1142.97 | loss  0.53 | ppl     1.69\n",
      "test f1:  0.6386658431130328\n",
      "| epoch  34 | batch     5 | lr 0.00 | ms/batch 1053.66 | loss  0.51 | ppl     1.66\n",
      "test f1:  0.6300653594771242\n",
      "| epoch  35 | batch     5 | lr 0.00 | ms/batch 1052.03 | loss  0.51 | ppl     1.67\n",
      "test f1:  0.6365302382406842\n",
      "| epoch  36 | batch     5 | lr 0.00 | ms/batch 1057.11 | loss  0.51 | ppl     1.67\n",
      "test f1:  0.6337499999999999\n",
      "| epoch  37 | batch     5 | lr 0.00 | ms/batch 2431.18 | loss  0.51 | ppl     1.66\n",
      "test f1:  0.6391494684177611\n",
      "| epoch  38 | batch     5 | lr 0.00 | ms/batch 2579.92 | loss  0.51 | ppl     1.66\n",
      "test f1:  0.6394293125810636\n",
      "| epoch  39 | batch     5 | lr 0.00 | ms/batch 2579.50 | loss  0.51 | ppl     1.66\n",
      "test f1:  0.643878185208204\n",
      "| epoch  40 | batch     5 | lr 0.00 | ms/batch 2683.21 | loss  0.49 | ppl     1.64\n",
      "test f1:  0.6455294863665187\n",
      "| epoch  41 | batch     5 | lr 0.00 | ms/batch 2333.94 | loss  0.50 | ppl     1.64\n",
      "test f1:  0.6487523992322457\n",
      "| epoch  42 | batch     5 | lr 0.00 | ms/batch 3156.91 | loss  0.50 | ppl     1.65\n",
      "test f1:  0.6537467700258398\n",
      "| epoch  43 | batch     5 | lr 0.00 | ms/batch 2321.33 | loss  0.49 | ppl     1.64\n",
      "test f1:  0.6493827160493827\n",
      "| epoch  44 | batch     5 | lr 0.00 | ms/batch 2545.71 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6478318002628122\n",
      "| epoch  45 | batch     5 | lr 0.00 | ms/batch 1464.90 | loss  0.49 | ppl     1.64\n",
      "test f1:  0.6466992665036675\n",
      "| epoch  46 | batch     5 | lr 0.00 | ms/batch 1223.97 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6391478029294275\n",
      "| epoch  47 | batch     5 | lr 0.00 | ms/batch 1532.56 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6416831032215649\n",
      "| epoch  48 | batch     5 | lr 0.00 | ms/batch 1768.03 | loss  0.48 | ppl     1.62\n",
      "test f1:  0.6453232893910861\n",
      "| epoch  49 | batch     5 | lr 0.00 | ms/batch 1142.16 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6470211402946829\n",
      "| epoch  50 | batch     5 | lr 0.00 | ms/batch 1163.50 | loss  0.49 | ppl     1.64\n",
      "test f1:  0.6447028423772609\n",
      "| epoch  51 | batch     5 | lr 0.00 | ms/batch 1201.38 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6508432229856339\n",
      "| epoch  52 | batch     5 | lr 0.00 | ms/batch 1166.56 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6413838988689288\n",
      "| epoch  53 | batch     5 | lr 0.00 | ms/batch 1139.85 | loss  0.49 | ppl     1.64\n",
      "test f1:  0.654040404040404\n",
      "| epoch  54 | batch     5 | lr 0.00 | ms/batch 1151.03 | loss  0.48 | ppl     1.62\n",
      "test f1:  0.6537295330503335\n",
      "| epoch  55 | batch     5 | lr 0.00 | ms/batch 1173.01 | loss  0.48 | ppl     1.62\n",
      "test f1:  0.6514954486345904\n",
      "| epoch  56 | batch     5 | lr 0.00 | ms/batch 1193.42 | loss  0.48 | ppl     1.62\n",
      "test f1:  0.6537216828478963\n",
      "| epoch  57 | batch     5 | lr 0.00 | ms/batch 1177.16 | loss  0.48 | ppl     1.61\n",
      "test f1:  0.654911838790932\n",
      "| epoch  58 | batch     5 | lr 0.00 | ms/batch 1164.86 | loss  0.48 | ppl     1.62\n",
      "test f1:  0.6543130990415336\n",
      "| epoch  59 | batch     5 | lr 0.00 | ms/batch 1159.64 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6515837104072398\n",
      "| epoch  60 | batch     5 | lr 0.00 | ms/batch 1062.50 | loss  0.48 | ppl     1.61\n",
      "test f1:  0.6544535691724573\n",
      "| epoch  61 | batch     5 | lr 0.00 | ms/batch 1013.89 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.650925335035099\n",
      "| epoch  62 | batch     5 | lr 0.00 | ms/batch 1000.49 | loss  0.48 | ppl     1.62\n",
      "test f1:  0.6501950585175553\n",
      "| epoch  63 | batch     5 | lr 0.00 | ms/batch 1026.13 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6495176848874598\n",
      "| epoch  64 | batch     5 | lr 0.00 | ms/batch 1034.83 | loss  0.46 | ppl     1.59\n",
      "test f1:  0.6489980607627666\n",
      "| epoch  65 | batch     5 | lr 0.00 | ms/batch 1002.25 | loss  0.47 | ppl     1.61\n",
      "test f1:  0.6512226512226513\n",
      "| epoch  66 | batch     5 | lr 0.00 | ms/batch 1029.37 | loss  0.47 | ppl     1.59\n",
      "test f1:  0.6474358974358975\n",
      "| epoch  67 | batch     5 | lr 0.00 | ms/batch 1008.48 | loss  0.49 | ppl     1.63\n",
      "test f1:  0.6467218332272439\n",
      "| epoch  68 | batch     5 | lr 0.00 | ms/batch 1012.44 | loss  0.48 | ppl     1.61\n",
      "test f1:  0.6477946017116524\n",
      "| epoch  69 | batch     5 | lr 0.00 | ms/batch 1064.52 | loss  0.47 | ppl     1.60\n",
      "test f1:  0.648051948051948\n",
      "| epoch  70 | batch     5 | lr 0.00 | ms/batch 1030.62 | loss  0.47 | ppl     1.61\n",
      "test f1:  0.6470588235294119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20348\\3727767672.py\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mf1_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20348\\3727767672.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# 反向傳播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0\n",
    "    log_interval = 5\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, (train_data, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # 轉float\n",
    "        output = model.forward(train_data)\n",
    "        targets = torch.eye(2)[labels.long()]\n",
    "\n",
    "        # 計算損失\n",
    "        loss = criterion(output, targets)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "\n",
    "        # 反向傳播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if batch % log_interval == 0 and batch > 1:\n",
    "\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | batch {(batch):5d} | '\n",
    "                    f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                    f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_test(model):\n",
    "    model.eval()\n",
    "    y_pred = torch.argmax(model.forward(X_test.to_list()), dim=1)\n",
    "    f1 = f1_score(y_test, y_pred.detach().numpy())\n",
    "    print(\"test f1: \",f1)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    f1_test(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

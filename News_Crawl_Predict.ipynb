{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5908df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "from time import sleep\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1545bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "from time import sleep\n",
    "from typing import List\n",
    "### CLASSEs\n",
    "class GoogleNews:\n",
    "\n",
    "    def __init__(self, account:str, password:str, proxy_list=List[str],lang=\"en\", period=\"\", encode=\"utf-8\", region=None):\n",
    "        self.account = account\n",
    "        self.password = password\n",
    "        self.proxy_list = proxy_list\n",
    "        self.user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0'\n",
    "        self.__lang = lang\n",
    "        # 若user提供region，則headers加入accept-language，否則不加\n",
    "        if region:\n",
    "            self.accept_language= lang + '-' + region + ',' + lang + ';q=0.9'\n",
    "            self.headers = {'User-Agent': self.user_agent, 'Accept-Language': self.accept_language}\n",
    "        else:\n",
    "            self.headers = {'User-Agent': self.user_agent}\n",
    "        self.__period = period\n",
    "        self.__encode = encode\n",
    "        self.proxy = self.proxy_check(proxy_list=self.proxy_list)\n",
    "        self.__results = self.result_df_init()\n",
    "\n",
    "    def response_builder(self, url, proxy, is_google_news_search=False):\n",
    "        \"\"\"\n",
    "        將proxy設置於opener，並回傳req\n",
    "        is_google_news_search: 搜尋goolge news時要設為True\n",
    "        \"\"\"\n",
    "        if is_google_news_search:\n",
    "            self.req = urllib.request.Request(self.url.replace(\"search?\",\"search?hl=\"+self.__lang+\"&gl=\"+self.__lang+\"&\"), headers=self.headers)\n",
    "        else:\n",
    "            self.req = urllib.request.Request(url, headers=self.headers)\n",
    "\n",
    "        self.proxy = proxy\n",
    "        self.proxy_support = urllib.request.ProxyHandler(\n",
    "                            {\n",
    "                                'http':f'http://{self.account}:{self.password}@{proxy}', \n",
    "                                'https':f'https://{self.account}:{self.password}@{proxy}',\n",
    "                            })\n",
    "\n",
    "        opener = urllib.request.build_opener(self.proxy_support)\n",
    "        urllib.request.install_opener(opener)\n",
    "        self.response = urllib.request.urlopen(self.req, timeout=10)\n",
    "\n",
    "        return self.response\n",
    "\n",
    "    def proxy_check(self, proxy_list):\n",
    "        \"\"\"\n",
    "        有時公司個別proxy會失效\n",
    "        讀取所有的proxy，嘗試有用的proxy，並回傳第一個有用的proxy\n",
    "        \"\"\"\n",
    "        self.work_proxies = []\n",
    "        test_url = \"https://www.google.com/?hl=zh_TW\"\n",
    "\n",
    "        retries = 3 # 每個proxy_list中的proxy嘗試次數\n",
    "        print('proxy check')\n",
    "        for proxy in proxy_list:\n",
    "            print('proxy= ', proxy, end='')\n",
    "            for attempt in range(retries):\n",
    "                if self.work_proxies==[]:\n",
    "                    try:\n",
    "                        response = self.response_builder(url=test_url, proxy=proxy, is_google_news_search=False)\n",
    "                        # 成功連線\n",
    "                        response.close()\n",
    "                        self.work_proxies.append(proxy)\n",
    "                        print(\" ok\")\n",
    "                    except Exception as e:\n",
    "                        print(f' failed (Exception: {e}')\n",
    "                        sleep(3)\n",
    "\n",
    "        if self.work_proxies == []: \n",
    "            print(\"目前並無有效proxy\")\n",
    "        else:\n",
    "            self.proxy = self.work_proxies[0]\n",
    "            return self.proxy\n",
    "\n",
    "    def utc_to_taipei(self, date: str):\n",
    "        \n",
    "        try:\n",
    "            utc = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%SZ')\n",
    "            from_zone = tz.gettz('UTC')\n",
    "            to_zone = tz.gettz('Asia/Taipei')\n",
    "            # Tell the datetime object that it's in UTC time zone since \n",
    "            # datetime objects are 'naive' by default\n",
    "            utc = utc.replace(tzinfo=from_zone)\n",
    "            taipei = utc.astimezone(to_zone)\n",
    "            taipei_strtime = taipei.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            return taipei_strtime\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        \n",
    "    def result_df_init(self):\n",
    "        results_col = ['title', 'desc', 'date', 'datetime', 'link', 'img', 'media', 'site', 'keyword', 'category']\n",
    "        self.__results = pd.DataFrame(columns=results_col)\n",
    "        return self.__results\n",
    "\n",
    "    def get_news(self, key=\"\", key_catogory=\"\", deamplify=False):\n",
    "        \n",
    "        my_key = key\n",
    "\n",
    "        if self.__encode != \"\":\n",
    "            key = urllib.request.quote(key.encode(self.__encode))\n",
    "\n",
    "        if key == \"\":\n",
    "            self.url = 'https://news.google.com/?hl={}'.format(self.__lang)\n",
    "        else:\n",
    "            key = \"+\".join(key.split(\" \")) \n",
    "            self.url = 'https://news.google.com/search?q={}+when:{}&hl={}'.format( key, self.__period, self.__lang.lower())\n",
    "\n",
    "    \n",
    "        retries = 3 # 嘗試連線次數\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                self.response = self.response_builder(url=self.url, proxy=self.proxy, is_google_news_search=True)\n",
    "                self.page = self.response.read()\n",
    "                self.content = Soup(self.page, \"html.parser\")\n",
    "                self.response.close()    \n",
    "                break\n",
    "            except Exception as e:  # 連線錯誤，articles尚未迭帶\n",
    "                print(e)\n",
    "                print(f\"搜尋時連線錯誤: 關鍵字-{my_key}(等待10秒後重試)\")\n",
    "                sleep(10)\n",
    "                self.response = None\n",
    "                self.proxy = self.proxy_check(proxy_list=self.proxy_list)\n",
    "                \n",
    "\n",
    "        if self.response is None:\n",
    "            print(f\"搜尋時連線錯誤: 停止搜關鍵字-{my_key}\")\n",
    "            return\n",
    "        \n",
    "        elif self.response is not None:           \n",
    "            # 開始解析文章\n",
    "            articles = self.content.select('div[class=\"NiLAwe y6IFtc R7GTQ keNKEd j7vNaf nID9nc\"]')\n",
    "            for article in articles:\n",
    "                try:\n",
    "\n",
    "                    # title\n",
    "                    try:\n",
    "                        title=article.find('h3').text\n",
    "                    except:\n",
    "                        title=None\n",
    "\n",
    "                    # description\n",
    "                    try:\n",
    "                        desc=article.find('span').text\n",
    "                    except:\n",
    "                        desc=None\n",
    "\n",
    "                    # date\n",
    "                    try:\n",
    "                        date = article.find(\"time\").text\n",
    "                    except:\n",
    "                        date = None\n",
    "\n",
    "                    # datetime\n",
    "                    try:\n",
    "                        datetime=article.find('time').get('datetime')\n",
    "                        datetime_chars=self.utc_to_taipei(datetime)\n",
    "                    except:\n",
    "                        datetime_chars=None \n",
    "\n",
    "                    # link\n",
    "                    if not deamplify:\n",
    "                        link = 'news.google.com/' + article.find(\"h3\").find(\"a\").get(\"href\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            link = 'news.google.com/' + article.find(\"h3\").find(\"a\").get(\"href\")\n",
    "                        except Exception as deamp_e:\n",
    "                            print(deamp_e)\n",
    "                            link = article.find(\"article\").get(\"jslog\").split('2:')[1].split(';')[0]\n",
    "\n",
    "                    if link.startswith('https://www.youtube.com/watch?v='):\n",
    "                        desc = 'video'\n",
    "\n",
    "                    # image\n",
    "                    try:\n",
    "                        img = article.find(\"img\").get(\"src\")\n",
    "                    except:\n",
    "                        img = None\n",
    "\n",
    "                    # site\n",
    "                    try:\n",
    "                        site=article.select('a[class*=\"wEwyrc\"]')[0].text\n",
    "                    except:\n",
    "                        site=None\n",
    "\n",
    "                    # collection\n",
    "                    data = [title, desc, date, datetime_chars, link, img, None, site, my_key, key_catogory]\n",
    "                    self.__results.loc[len(self.__results)] = data\n",
    "                \n",
    "                    \n",
    "                # 解析錯誤\n",
    "                except Exception as e_article:                   \n",
    "                    print(e_article, f\"{my_key}一篇文章解析錯誤\")\n",
    "                    return None\n",
    "\n",
    "            return self.__results\n",
    "        \n",
    "    def results(self,sort=False):\n",
    "        \"\"\"Returns the __results.\n",
    "        New feature: include datatime and sort the articles in decreasing order\"\"\"\n",
    "        results=self.__results\n",
    "        if sort:\n",
    "            try:\n",
    "                results=results.sort_values(by='datetime',ascending=False)\n",
    "            except:\n",
    "                print(\"No datetime column\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b08e0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google_news_crawl import GoogleNews\n",
    "import pyshorteners\n",
    "import requests\n",
    "import hanzidentifier\n",
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "### 分詞用 ###\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger\n",
    "\n",
    "### 讀取模型用 ###\n",
    "import dill\n",
    "\n",
    "class crawler_data_getter:\n",
    "\n",
    "    ### set parameters\n",
    "    # 建立爬蟲物件: 連到外網需要proxy才能連，而proxy中要打入nt帳密\n",
    "    proxy_list = pd.read_excel(\"./crawler_config/proxy.xlsx\", header=None)[0].tolist()\n",
    "    nt_account = pd.read_excel(\"./crawler_config/account.xlsx\")\n",
    "    auo_account = nt_account[\"NTaccount\"][0]\n",
    "    auo_password = nt_account[\"password\"][0]\n",
    "    period = '1d'\n",
    "    googlenews = GoogleNews(lang='zh-TW', period=period, account=auo_account, password=auo_password, proxy_list=proxy_list)\n",
    "    # 搜尋詞列表: 爬蟲會先用預設好的關鍵字列表去搜尋新聞，並在後續會依照關鍵字所屬的類別來分類新聞\n",
    "    key_words_df = pd.read_excel(\"./crawler_config/keyword.xlsx\")\n",
    "    # 停用標題列表\n",
    "    drop_list_path = \"./crawler_config/dropword.xlsx\"\n",
    "    drop_series = pd.read_excel(drop_list_path, header=None)[0]\n",
    "    # 出處列表\n",
    "    site_list_path = \"./crawler_config/NewsSite.xlsx\"\n",
    "    site_series = pd.read_excel(site_list_path)[\"出處\"].str.replace(\" \",\"\").str.lower()\n",
    "\n",
    "    # 短網址用\n",
    "    link_col_name = \"link\"\n",
    "\n",
    "    # 分詞用\n",
    "    # ckip分詞模型父資料夾\n",
    "    ckip_path = \".\\\\ckiplab\\\\\"\n",
    "    ws_model = 'albert-base-chinese-ws'\n",
    "    pos_model = 'albert-base-chinese-pos'\n",
    "    title_col_name = \"title\"\n",
    "\n",
    "    # 爬蟲用\n",
    "    keyword_col_name = \"keyword\" #關鍵字欄位名\n",
    "    category_col_name = \"category\" #關鍵字類別欄位名\n",
    "\n",
    "    # 預測用\n",
    "    # 預測變數名稱\n",
    "    variable_names = [\"click\", \"DailyNews\"]\n",
    "    # 模型路徑\n",
    "    model_path = \"./models/\"\n",
    "    # 從爬蟲表格選的部分欄位\n",
    "    selected_col = ['title', 'datetime', 'link', 'short_link', 'site', 'category']\n",
    "\n",
    "    ### 開始爬蟲\n",
    "    def start_crawl(self):\n",
    "        \"\"\"\n",
    "        用爬蟲物件做爬資料的動作\n",
    "        \"\"\"\n",
    "        tqdm.pandas(desc=\"用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊\")\n",
    "        search_action = self.key_words_df.progress_apply(lambda row: self.googlenews.get_news(key=row[self.keyword_col_name], key_catogory=row[self.category_col_name]), axis=1)\n",
    "\n",
    "    def get_crawl_result(self):\n",
    "        \"\"\"\n",
    "        爬蟲結束後，取出爬到的結果\n",
    "        \"\"\"\n",
    "        self.search_result_df = self.googlenews.results()\n",
    "\n",
    "    def crawl_data_cleaner(self):\n",
    "        \"\"\"\n",
    "        篩選資料\n",
    "        \"\"\"\n",
    "        # 篩選掉有停用的標題、出處的資料\n",
    "        print(\"篩選掉不可用的標題、不可用的新聞出處\")\n",
    "        \n",
    "        def title_site_filter(row:pd.Series, drop_series:pd.Series, site_series:pd.Series, title_name:str=\"title\", site_name:str=\"site\"):\n",
    "            \"\"\"\n",
    "            row: 搜尋結果的一列\n",
    "            drop_series: 停用標題列表\n",
    "            site_series: 出處列表\n",
    "            title_name: 標題欄位名\n",
    "            site_name: 出處欄位名\n",
    "            \"\"\"\n",
    "            title  = row[title_name]\n",
    "            site = row[site_name]\n",
    "\n",
    "            # 只保留含有關鍵字的標題\n",
    "            condition_1 = (hanzidentifier.is_traditional(title))\n",
    "            \n",
    "            # 只保留標題是繁體中文，或標題含有'彭双浪'的文章\n",
    "            condition_2 = (\"彭双浪\" in title) \n",
    "\n",
    "            # 只保留標題內不含dropword.xlsx指定文字的文章\n",
    "            condition_3 = drop_series.apply(lambda drop_word: drop_word not in title).all()\n",
    "\n",
    "            # 只保留來源自指定的新聞出處的文章\n",
    "            condition_4 = site in site_series.to_list()\n",
    "\n",
    "            return ( condition_1 or condition_2) and condition_3 and condition_4\n",
    "\n",
    "        row_filter = self.search_result_df.apply(lambda row: title_site_filter( row, drop_series=self.drop_series, site_series=self.site_series), axis=1)\n",
    "        self.search_result_df = self.search_result_df[row_filter]\n",
    "\n",
    "        # 排除link欄位重複的資料，保留第一筆\n",
    "        print(\"篩選掉重複資料\")\n",
    "        self.search_result_df.drop_duplicates(subset='link', keep=\"first\", inplace=True)\n",
    "\n",
    "        # 排除title、datetime、site三個欄位為整體，都重複才排除\n",
    "        # search_result_df.drop_duplicates(subset=['title','datetime','site'], keep='first', inplace=True)\n",
    "\n",
    "        # 排除title重複\n",
    "        self.search_result_df.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "\n",
    "        # 依照catogory來排序\n",
    "        print(\"依新聞類別排序資料\")\n",
    "        self.search_result_df = (self.search_result_df\n",
    "                                    .assign(**{\"sort\": lambda search_result_df: self.search_result_df[self.category_col_name] # 新增sort欄位，sort的值依照catogory來排序編號\n",
    "                                                                                .apply(lambda catogory: {'產業':0, '法令':1, 'HR':2}[catogory] if catogory in {'產業', '法令', 'HR'} else 3) \n",
    "                                        }\n",
    "                                    )\n",
    "                                    .sort_values([\"sort\"]) # 依照sort欄位的數值大小來排序\n",
    "                                    .drop(columns=[\"sort\"])) # 去除sort欄位\n",
    "\n",
    "    def news_link_shorten(self):\n",
    "        \"\"\"\n",
    "        將爬蟲結果中的新聞連結縮短網址\n",
    "        \"\"\"\n",
    "        ### 縮短爬蟲資料網址\n",
    "        def url_shorten(auo_account:str, auo_password:str, proxy:str, url:str):\n",
    "            \"\"\"\n",
    "            apply用\n",
    "            auo_account: auo帳號\n",
    "            auo_password: auo密碼\n",
    "            proxy: proxy\n",
    "            url: 要縮短的網址\n",
    "            傳入要縮短的網址，回傳短網址\n",
    "            \"\"\"\n",
    "\n",
    "            # 若url不是以\"http://\"或\"https://\"開頭，則加上\"http://\"\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = f\"http://{url}\"\n",
    "\n",
    "            # 加入proxy\n",
    "            session = pyshorteners.Shortener(\n",
    "                proxies={\n",
    "                    'http':f'http://{auo_account}:{auo_password}@{proxy}',\n",
    "                    'https':f'http://{auo_account}:{auo_password}@{proxy}',\n",
    "                },\n",
    "                timeout=10 # 設置超時時間為10秒\n",
    "            )\n",
    "\n",
    "            # 縮短網址: 若連線超過10秒未回應，則等待10秒後重試，最多重試三次\n",
    "            while True:\n",
    "                patience = 3\n",
    "                try:\n",
    "                    short_url = session.tinyurl.short(url)\n",
    "                    return short_url\n",
    "                except requests.exceptions.Timeout:\n",
    "                    print(\"錯誤: 短網址請求超過10秒未回應\")\n",
    "                    print(\"等待10秒後重試\")\n",
    "                    sleep(10)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"發生連線錯誤: {e}\")\n",
    "                    print(\"等待10秒後重試\")\n",
    "                    sleep(10)\n",
    "                except Exception:\n",
    "                    print(\"縮短一個連結時發生連線外錯誤，等待10秒後重試\")\n",
    "                    sleep(10)\n",
    "                    patience += 1 \n",
    "                    if patience >= 3:\n",
    "                        print(\"已重試三次，跳過此連結\")\n",
    "                        return None\n",
    "\n",
    "        tqdm.pandas(desc=\"縮短所有Google News連結\")\n",
    "        self.search_result_df['short_link'] = self.search_result_df[self.link_col_name].progress_apply(lambda link: url_shorten(auo_account=self.auo_account, auo_password=self.auo_password, proxy=self.googlenews.proxy, url=link))\n",
    "\n",
    "    def titles_tokenizer(self):\n",
    "        \"\"\"\n",
    "        讀ckip模型對新聞標題做分詞並去除非名詞、非動詞\n",
    "        返回一個處理過的字串串列\n",
    "        \"\"\"      \n",
    "        \n",
    "        # 建立分詞、詞性標記、命名實體物件\n",
    "        ws_driver = CkipWordSegmenter(model_name=self.ckip_path + self.ws_model) # 分詞\n",
    "        pos_driver = CkipPosTagger(model_name=self.ckip_path + self.pos_model)    # 詞性標記(POS)\n",
    "\n",
    "        # 開始分詞、詞性標記，分詞結果放入ws，詞性標註結果放入pos        \n",
    "        ws = ws_driver(self.search_result_df[self.title_col_name].tolist())\n",
    "        pos = pos_driver(ws)\n",
    "\n",
    "        print(\"去除非名詞、非動詞\")\n",
    "        # 用來保留主要詞彙的函式\n",
    "        def clean(sentence_ws, sentence_pos): \n",
    "\n",
    "            short_sentence = []\n",
    "            stop_pos = set(['Nep', 'Nh']) # 這 2種詞性不保留(指代定詞、代名詞)\n",
    "\n",
    "            for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "\n",
    "                # 部分的分詞內有包含空白字元，會導致停用詞不會被辨識到，所以在這邊用replace處理掉空白字元\n",
    "                word_ws = word_ws.replace(' ','')\n",
    "\n",
    "                # 只留名詞和動詞\n",
    "                is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\") or word_pos.startswith(\"FW\")\n",
    "                # 去掉名詞裡的某些詞性(指代定詞、代名詞、專有名詞)\n",
    "                is_not_stop_pos = word_pos not in stop_pos\n",
    "                # 只剩一個字的詞也不留\n",
    "                is_not_one_charactor = not (len(word_ws) == 1)\n",
    "\n",
    "                # 組成串列\n",
    "                if is_N_or_V and is_not_stop_pos and is_not_one_charactor:\n",
    "                    short_sentence.append(f\"{word_ws}\")\n",
    "\n",
    "            return \" \".join(short_sentence)\n",
    "\n",
    "\n",
    "        self.cleaned_title_list = pd.DataFrame({'ws':pd.Series(ws), 'pos':pd.Series(pos)}).apply(lambda row: clean(row['ws'], row['pos']), axis=1).tolist()\n",
    "\n",
    "\n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        讀預測click、預測dailynews的模型，並對ckip處理後的標題做預測，並將預測機率、分類等，合併進原本的dataframe\n",
    "        返回所需的最終表格\n",
    "        \"\"\"\n",
    "        \n",
    "        # 讀取模型\n",
    "        pred_result_dic = dict()\n",
    "        for name in self.variable_names:\n",
    "            model_filename = f\"model_{name}_best_f1.pkl\"\n",
    "\n",
    "            print(f\"{name}模型預測\")\n",
    "\n",
    "            # 開啟模型\n",
    "            with open( self.model_path + model_filename, \"rb\") as f:\n",
    "                model = dill.load(f)\n",
    "\n",
    "            # 提供預測結果及其機率值\n",
    "            pred = model.predict(self.cleaned_title_list )\n",
    "            prob = model.predict_proba(self.cleaned_title_list )[:,1]\n",
    "\n",
    "            # 前面content_df的title在分詞前被處理過\n",
    "            pred_result_dic[f'{name}_prob'] = prob\n",
    "            pred_result_dic[f'{name}_prediction'] = pred\n",
    "\n",
    "        pred_result = pd.DataFrame( pred_result_dic, columns=[\"click_prob\", \"DailyNews_prob\", 'click_prediction', \"DailyNews_prediction\"]) \n",
    "\n",
    "        # 將預測結果合併進dataframe\n",
    "        print(\"預測表格彙整\")\n",
    "        # 建立包含預測結果的pred_df\n",
    "        pred_df = pd.concat([self.search_result_df[self.selected_col].reset_index(drop=True), pred_result], axis=1)\n",
    "\n",
    "        # # 依照類別切出子表\n",
    "        categories = pred_df.category.unique().tolist()\n",
    "        sub_result_df_list = []\n",
    "        for category in categories:\n",
    "            sub_df_mask = pred_df.category == category\n",
    "            sub_df = pred_df[sub_df_mask].copy()\n",
    "\n",
    "            # 以機率值排序，讓機率高的優先排在前面\n",
    "            sub_df[\"total_prob\"] = sub_df[\"click_prob\"] + sub_df[\"DailyNews_prob\"]\n",
    "            sub_df = sub_df.sort_values(by=[\"total_prob\"], ascending=False)\n",
    "            sub_df = sub_df.drop(columns=[\"total_prob\"])\n",
    "\n",
    "            # 彙整子表\n",
    "            sub_result_df_list.append(sub_df)\n",
    "\n",
    "        # # 合併子表\n",
    "        self.pred_df = pd.concat(sub_result_df_list)\n",
    "\n",
    "        # # # 寫出\n",
    "        print(\"寫出預測表格\")\n",
    "        today = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "        pred_df.to_excel(f\".\\\\history_predict\\\\News_{today}_predict.xlsx\", encoding='utf-8-sig', index=False)\n",
    "\n",
    "    def main(self):\n",
    "        self.start_crawl()\n",
    "        self.get_crawl_result()\n",
    "        ### 整理爬蟲資料\n",
    "        print(\"開始整理爬蟲資料\")\n",
    "        self.crawl_data_cleaner()\n",
    "        self.news_link_shorten()\n",
    "        print(\"開始對標題分詞\")\n",
    "        self.titles_tokenizer()\n",
    "        # ### **讀取模型**\n",
    "        print(\"讀取預測模型\") \n",
    "        self.model_predict()  \n",
    "        print(\"爬蟲及預測完成\")\n",
    "        \n",
    "        return self.pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa56738",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obj = crawler_data_getter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05cf50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:   2%|▏         | 2/82 [00:06<04:09,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:   5%|▍         | 4/82 [00:27<09:22,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:   6%|▌         | 5/82 [00:33<08:33,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " failed (Exception: <urlopen error timed out>\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:   7%|▋         | 6/82 [00:55<14:48, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  10%|▉         | 8/82 [01:23<14:55, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  15%|█▍        | 12/82 [01:43<07:20,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  27%|██▋       | 22/82 [02:33<04:27,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  28%|██▊       | 23/82 [02:38<04:43,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  30%|███       | 25/82 [03:02<07:31,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  33%|███▎      | 27/82 [03:18<07:10,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      " failed (Exception: <urlopen error timed out>\n",
      "proxy=  auhqproxy.corpnet.auo.com:8083 failed (Exception: <urlopen error Tunnel connection failed: 501 Not Implemented>\n",
      " failed (Exception: <urlopen error Tunnel connection failed: 501 Not Implemented>\n",
      " failed (Exception: <urlopen error Tunnel connection failed: 501 Not Implemented>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  34%|███▍      | 28/82 [04:12<19:24, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前並無有效proxy\n",
      "搜尋時連線錯誤\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: The read operation timed out\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  35%|███▌      | 29/82 [04:39<20:43, 23.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: <urlopen error timed out>\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  41%|████▏     | 34/82 [05:26<07:13,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  49%|████▉     | 40/82 [05:56<03:11,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  52%|█████▏    | 43/82 [06:18<03:40,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  57%|█████▋    | 47/82 [06:45<03:49,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  61%|██████    | 50/82 [07:03<03:01,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  62%|██████▏   | 51/82 [07:15<03:48,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: The read operation timed out\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080 failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: <urlopen error timed out>\n",
      " ok\n",
      "proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  63%|██████▎   | 52/82 [07:55<08:36, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  65%|██████▍   | 53/82 [08:11<08:06, 16.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  68%|██████▊   | 56/82 [08:28<04:09,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  71%|███████   | 58/82 [08:45<03:27,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  73%|███████▎  | 60/82 [09:20<04:29, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  74%|███████▍  | 61/82 [09:32<04:12, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: <urlopen error timed out>\n",
      " failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  76%|███████▌  | 62/82 [09:55<05:08, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error timed out>\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  79%|███████▉  | 65/82 [10:12<02:29,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  84%|████████▍ | 69/82 [10:43<01:37,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      " ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  95%|█████████▌| 78/82 [11:51<00:25,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      " failed (Exception: The read operation timed out\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊:  99%|█████████▉| 81/82 [12:35<00:09,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n",
      "proxy check\n",
      "proxy=  auhqproxy.cdn.corpnet.auo.com:8080 ok\n",
      "proxy=  auhqwsg.corpnet.auo.com:8080proxy=  auhqproxy.corpnet.auo.com:8083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "用所有關鍵字去爬取一天內的Google News，並紀錄標題、描述、日期...等資訊: 100%|██████████| 82/82 [12:50<00:00,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始整理爬蟲資料\n",
      "篩選掉不可用的標題、不可用的新聞出處\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenBLLee\\AppData\\Local\\Temp\\ipykernel_27792\\2179601521.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.search_result_df.drop_duplicates(subset='link', keep=\"first\", inplace=True)\n",
      "C:\\Users\\BenBLLee\\AppData\\Local\\Temp\\ipykernel_27792\\2179601521.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.search_result_df.drop_duplicates(subset=['title'], keep='first', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "篩選掉重複資料\n",
      "依新聞類別排序資料\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "縮短所有Google News連結:  51%|█████▏    | 202/393 [01:38<01:46,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "縮短一個連結時發生連線外錯誤，等待10秒後重試\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "縮短所有Google News連結:  52%|█████▏    | 203/393 [01:48<10:53,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已重試三次，跳過此連結\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "縮短所有Google News連結: 100%|██████████| 393/393 [03:42<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始對標題分詞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 393/393 [00:00<00:00, 25156.99it/s]\n",
      "Inference: 100%|██████████| 2/2 [00:15<00:00,  7.95s/it]\n",
      "Tokenization: 100%|██████████| 393/393 [00:00<00:00, 39548.02it/s]\n",
      "Inference: 100%|██████████| 3/3 [00:20<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除非名詞、非動詞\n",
      "讀取預測模型\n",
      "click模型預測\n",
      "DailyNews模型預測\n",
      "預測表格彙整\n",
      "寫出預測表格\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenBLLee\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py:211: FutureWarning: the 'encoding' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'encoding'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬蟲及預測完成\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>datetime</th>\n",
       "      <th>link</th>\n",
       "      <th>short_link</th>\n",
       "      <th>site</th>\n",
       "      <th>category</th>\n",
       "      <th>click_prob</th>\n",
       "      <th>DailyNews_prob</th>\n",
       "      <th>click_prediction</th>\n",
       "      <th>DailyNews_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>友達以上戀人未滿？ 六旬婦傳自拍修圖美照網戀差點遭詐</td>\n",
       "      <td>2023-11-02 09:10:09</td>\n",
       "      <td>news.google.com/./articles/CBMiJ2h0dHBzOi8vdWR...</td>\n",
       "      <td>https://tinyurl.com/ykwahjvw</td>\n",
       "      <td>聯合新聞網</td>\n",
       "      <td>產業</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.879778</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>華邦電看半導體隧道透光Q4勝Q3明年再向上| 產經</td>\n",
       "      <td>2023-11-02 03:56:36</td>\n",
       "      <td>news.google.com/./articles/CBMiMGh0dHBzOi8vbmV...</td>\n",
       "      <td>https://tinyurl.com/yterkvua</td>\n",
       "      <td>非凡新聞</td>\n",
       "      <td>產業</td>\n",
       "      <td>0.686473</td>\n",
       "      <td>0.759857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>全球首個漂浮式風漁融合發電完工可讓6千多家戶用一年</td>\n",
       "      <td>2023-11-01 11:51:00</td>\n",
       "      <td>news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...</td>\n",
       "      <td>https://tinyurl.com/yw3ufqh6</td>\n",
       "      <td>工商時報</td>\n",
       "      <td>產業</td>\n",
       "      <td>0.733866</td>\n",
       "      <td>0.701341</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>日本半導體挾「2優勢」要抓回失去的40年！集邦：盤點未來可能形成 ...</td>\n",
       "      <td>2023-11-01 17:14:30</td>\n",
       "      <td>news.google.com/./articles/CBMiR2h0dHBzOi8vd3d...</td>\n",
       "      <td>https://tinyurl.com/yl3xesbk</td>\n",
       "      <td>財訊</td>\n",
       "      <td>產業</td>\n",
       "      <td>0.793341</td>\n",
       "      <td>0.641619</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>南科7個區都市計畫內政部審議通過 半導體生產完整基地</td>\n",
       "      <td>2023-11-01 19:16:35</td>\n",
       "      <td>news.google.com/./articles/CBMiLmh0dHBzOi8vbW9...</td>\n",
       "      <td>https://tinyurl.com/yvtfpjwa</td>\n",
       "      <td>經濟日報</td>\n",
       "      <td>產業</td>\n",
       "      <td>0.649125</td>\n",
       "      <td>0.766329</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>桃青年局首創2023 Taoyuan FuturePeak 桃園青創博覽會</td>\n",
       "      <td>2023-11-01 10:47:27</td>\n",
       "      <td>news.google.com/./articles/CBMiLmh0dHBzOi8vbW9...</td>\n",
       "      <td>https://tinyurl.com/ykre5tp9</td>\n",
       "      <td>經濟日報</td>\n",
       "      <td>HR</td>\n",
       "      <td>0.034195</td>\n",
       "      <td>0.015631</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>塑膠中心慶30助業界前瞻永續</td>\n",
       "      <td>2023-11-02 03:00:00</td>\n",
       "      <td>news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...</td>\n",
       "      <td>https://tinyurl.com/ymyuefgv</td>\n",
       "      <td>工商時報</td>\n",
       "      <td>HR</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>0.021787</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>學生打老師事件頻傳10縣市教師工會喊話教育部負起責任</td>\n",
       "      <td>2023-11-01 13:31:00</td>\n",
       "      <td>news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...</td>\n",
       "      <td>https://tinyurl.com/yk8pptg6</td>\n",
       "      <td>工商時報</td>\n",
       "      <td>HR</td>\n",
       "      <td>0.024773</td>\n",
       "      <td>0.016306</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>矽品志工國、台、英三聲道錄製有聲書 帶領惠明盲童沉浸式體驗學習</td>\n",
       "      <td>2023-11-01 23:04:05</td>\n",
       "      <td>news.google.com/./articles/CBMiNmh0dHBzOi8vbmV...</td>\n",
       "      <td>https://tinyurl.com/ywyn356r</td>\n",
       "      <td>自由時報</td>\n",
       "      <td>HR</td>\n",
       "      <td>0.028163</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>雲品酒店慶十五周年攜手台泥植林、朱宗慶樂團共創永續與音樂藝術 ...</td>\n",
       "      <td>2023-11-02 09:38:00</td>\n",
       "      <td>news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...</td>\n",
       "      <td>https://tinyurl.com/yqrsf4hc</td>\n",
       "      <td>工商時報</td>\n",
       "      <td>HR</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title             datetime  \\\n",
       "71              友達以上戀人未滿？ 六旬婦傳自拍修圖美照網戀差點遭詐  2023-11-02 09:10:09   \n",
       "35               華邦電看半導體隧道透光Q4勝Q3明年再向上| 產經  2023-11-02 03:56:36   \n",
       "49               全球首個漂浮式風漁融合發電完工可讓6千多家戶用一年  2023-11-01 11:51:00   \n",
       "38    日本半導體挾「2優勢」要抓回失去的40年！集邦：盤點未來可能形成 ...  2023-11-01 17:14:30   \n",
       "8               南科7個區都市計畫內政部審議通過 半導體生產完整基地  2023-11-01 19:16:35   \n",
       "..                                     ...                  ...   \n",
       "319  桃青年局首創2023 Taoyuan FuturePeak 桃園青創博覽會  2023-11-01 10:47:27   \n",
       "270                         塑膠中心慶30助業界前瞻永續  2023-11-02 03:00:00   \n",
       "317             學生打老師事件頻傳10縣市教師工會喊話教育部負起責任  2023-11-01 13:31:00   \n",
       "364        矽品志工國、台、英三聲道錄製有聲書 帶領惠明盲童沉浸式體驗學習  2023-11-01 23:04:05   \n",
       "287     雲品酒店慶十五周年攜手台泥植林、朱宗慶樂團共創永續與音樂藝術 ...  2023-11-02 09:38:00   \n",
       "\n",
       "                                                  link  \\\n",
       "71   news.google.com/./articles/CBMiJ2h0dHBzOi8vdWR...   \n",
       "35   news.google.com/./articles/CBMiMGh0dHBzOi8vbmV...   \n",
       "49   news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...   \n",
       "38   news.google.com/./articles/CBMiR2h0dHBzOi8vd3d...   \n",
       "8    news.google.com/./articles/CBMiLmh0dHBzOi8vbW9...   \n",
       "..                                                 ...   \n",
       "319  news.google.com/./articles/CBMiLmh0dHBzOi8vbW9...   \n",
       "270  news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...   \n",
       "317  news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...   \n",
       "364  news.google.com/./articles/CBMiNmh0dHBzOi8vbmV...   \n",
       "287  news.google.com/./articles/CBMiMmh0dHBzOi8vd3d...   \n",
       "\n",
       "                       short_link   site category  click_prob  DailyNews_prob  \\\n",
       "71   https://tinyurl.com/ykwahjvw  聯合新聞網       產業    0.970760        0.879778   \n",
       "35   https://tinyurl.com/yterkvua   非凡新聞       產業    0.686473        0.759857   \n",
       "49   https://tinyurl.com/yw3ufqh6   工商時報       產業    0.733866        0.701341   \n",
       "38   https://tinyurl.com/yl3xesbk     財訊       產業    0.793341        0.641619   \n",
       "8    https://tinyurl.com/yvtfpjwa   經濟日報       產業    0.649125        0.766329   \n",
       "..                            ...    ...      ...         ...             ...   \n",
       "319  https://tinyurl.com/ykre5tp9   經濟日報       HR    0.034195        0.015631   \n",
       "270  https://tinyurl.com/ymyuefgv   工商時報       HR    0.025398        0.021787   \n",
       "317  https://tinyurl.com/yk8pptg6   工商時報       HR    0.024773        0.016306   \n",
       "364  https://tinyurl.com/ywyn356r   自由時報       HR    0.028163        0.007241   \n",
       "287  https://tinyurl.com/yqrsf4hc   工商時報       HR    0.015256        0.008321   \n",
       "\n",
       "     click_prediction  DailyNews_prediction  \n",
       "71                  1                     1  \n",
       "35                  1                     1  \n",
       "49                  1                     1  \n",
       "38                  1                     1  \n",
       "8                   1                     1  \n",
       "..                ...                   ...  \n",
       "319                 0                     0  \n",
       "270                 0                     0  \n",
       "317                 0                     0  \n",
       "364                 0                     0  \n",
       "287                 0                     0  \n",
       "\n",
       "[393 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_obj.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99462170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
